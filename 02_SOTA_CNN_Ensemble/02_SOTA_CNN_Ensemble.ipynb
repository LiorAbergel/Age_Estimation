{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 02: SOTA CNN Ensemble\n",
    "\n",
    "## Overview\n",
    "This experiment evaluates industry-standard architectures (ResNet50, InceptionV3, InceptionResNetV2, DenseNet121, EfficientNetV2M) using Transfer Learning.\n",
    "\n",
    "**Key Features:**\n",
    "* **Patch-Based Approach:** High-resolution handwriting images are split into **400x400 patches** (stride 200) to capture local stroke details.\n",
    "* **Transfer Learning:** Weights pre-trained on ImageNet.\n",
    "* **Two-Stage Training:**\n",
    "    1.  **Phase 1:** Frozen backbone, train regression head (50 epochs, LR 1e-3).\n",
    "    2.  **Phase 2:** Unfreeze backbone, fine-tune (10 epochs, LR 1e-4).\n",
    "* **Ensemble Strategy:** Simple Average of predictions from all 5 models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4ShVeluU4Z2"
   },
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9mYBgch2fac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import (\n",
    "    ResNet50, InceptionV3, InceptionResNetV2, DenseNet121\n",
    ")\n",
    "from tensorflow.keras.applications.efficientnet_v2 import EfficientNetV2M\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# --- Configuration ---\n",
    "CONFIG = {\n",
    "    \"PATCH_SIZE\": (400, 400),\n",
    "    \"STRIDE\": 200,\n",
    "    \"BATCH_SIZE\": 128, # Reduced from README if GPU memory is tight, otherwise 128\n",
    "    \"EPOCHS_FROZEN\": 50,\n",
    "    \"EPOCHS_FINE_TUNE\": 10,\n",
    "    \"LR_FROZEN\": 1e-3,\n",
    "    \"LR_FINE_TUNE\": 1e-4,\n",
    "    \"DATA_DIR\": \"../data\",  \n",
    "    \"CSV_PATH\": \"../data/NewAgeSplit.csv\",\n",
    "    \"MODELS_DIR\": \"./models/experiment_02\",\n",
    "    \"AUGMENT\": True\n",
    "}\n",
    "\n",
    "os.makedirs(CONFIG[\"MODELS_DIR\"], exist_ok=True)\n",
    "print(f\"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGzNH3t4U97V"
   },
   "source": [
    "## 1. Data Processing: Dynamic Resizing & Patching\n",
    "Unlike the baseline, we cannot resize images to small dimensions (e.g., 128x128) without losing critical handwriting details. Instead, we:\n",
    "1.  **Resize** the image so the smaller dimension matches a standard size (800px), maintaining aspect ratio.\n",
    "2.  **Extract Patches** of size 400x400 with a stride of 200.\n",
    "3.  **Assign Labels**: Each patch inherits the age label of the parent image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aw7UOwd4BhwY"
   },
   "outputs": [],
   "source": [
    "def calculate_resized_dimensions(height, width, patch_size=400, stride=200, standard_size=800):\n",
    "    \"\"\"\n",
    "    Calculates dimensions to maintain aspect ratio and compatibility with patch extraction.\n",
    "    \"\"\"\n",
    "    aspect_ratio = width / height\n",
    "\n",
    "    # Scale so smaller side is standard_size\n",
    "    if height < width:\n",
    "        new_height = standard_size\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "    else:\n",
    "        new_width = standard_size\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "\n",
    "    # Adjust to ensure coverage by patches\n",
    "    def adjust_dimension(dim):\n",
    "        remainder = (dim - patch_size) % stride\n",
    "        return dim if remainder == 0 else dim - remainder\n",
    "\n",
    "    return adjust_dimension(new_height), adjust_dimension(new_width)\n",
    "\n",
    "def read_image_and_resize(img_path):\n",
    "    \"\"\"\n",
    "    Reads image using PIL, resizes dynamically, and normalizes to [0,1].\n",
    "    Wrapped in tf.py_function for use in TF pipeline.\n",
    "    \"\"\"\n",
    "    img = Image.open(img_path.numpy().decode(\"utf-8\"))\n",
    "    img = img.convert('RGB')\n",
    "    w, h = img.size\n",
    "    \n",
    "    new_h, new_w = calculate_resized_dimensions(h, w, \n",
    "                                                CONFIG[\"PATCH_SIZE\"][0], \n",
    "                                                CONFIG[\"STRIDE\"])\n",
    "    \n",
    "    img = img.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "    img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "    return img_array\n",
    "\n",
    "def process_image(row, data_dir, include_id=False):\n",
    "    \"\"\"\n",
    "    Loads image, extracts patches, and associates labels.\n",
    "    \"\"\"\n",
    "    img_path = tf.strings.join([data_dir, row['File']], separator=os.sep)\n",
    "\n",
    "    try:\n",
    "        # Use py_function to call PIL logic\n",
    "        img = tf.py_function(func=read_image_and_resize, inp=[img_path], Tout=tf.float32)\n",
    "        img.set_shape([None, None, 3]) \n",
    "    except Exception as e:\n",
    "        tf.print(f\"Error processing {img_path}\")\n",
    "        return None\n",
    "\n",
    "    # Extract Patches\n",
    "    patches = tf.image.extract_patches(\n",
    "        images=tf.expand_dims(img, 0),\n",
    "        sizes=[1, CONFIG[\"PATCH_SIZE\"][0], CONFIG[\"PATCH_SIZE\"][1], 1],\n",
    "        strides=[1, CONFIG[\"STRIDE\"], CONFIG[\"STRIDE\"], 1],\n",
    "        rates=[1, 1, 1, 1],\n",
    "        padding='VALID'\n",
    "    )\n",
    "    \n",
    "    # Reshape to (Num_Patches, 400, 400, 3)\n",
    "    patches = tf.reshape(patches, [-1, CONFIG[\"PATCH_SIZE\"][0], CONFIG[\"PATCH_SIZE\"][1], 3])\n",
    "    \n",
    "    # Replicate labels for all patches\n",
    "    labels = tf.fill([tf.shape(patches)[0]], row['Age'])\n",
    "    \n",
    "    if include_id:\n",
    "        ids = tf.fill([tf.shape(patches)[0]], row['File'])\n",
    "        return patches, labels, ids\n",
    "    \n",
    "    return patches, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTvo_ghY9Pei"
   },
   "outputs": [],
   "source": [
    "def create_dataset(data_dir, labels_df, dataset_type, augment=False, include_id=False):\n",
    "    \"\"\"\n",
    "    Creates a tf.data.Dataset that yields individual patches.\n",
    "    \"\"\"\n",
    "    subset_df = labels_df[labels_df['Set'] == dataset_type].reset_index(drop=True)\n",
    "    target_dir = os.path.join(data_dir, dataset_type)\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices(dict(subset_df))\n",
    "\n",
    "    # 1. Load Image & Extract Patches\n",
    "    if include_id:\n",
    "        ds = ds.map(lambda row: process_image(row, target_dir, include_id=True), \n",
    "                    num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        # Flatten: (Batch_of_Patches) -> (Patch)\n",
    "        ds = ds.flat_map(lambda patches, labels, ids: tf.data.Dataset.zip(\n",
    "            (tf.data.Dataset.from_tensor_slices(patches),\n",
    "             tf.data.Dataset.from_tensor_slices(labels),\n",
    "             tf.data.Dataset.from_tensor_slices(ids))\n",
    "        ))\n",
    "    else:\n",
    "        ds = ds.map(lambda row: process_image(row, target_dir, include_id=False), \n",
    "                    num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        ds = ds.flat_map(lambda patches, labels: tf.data.Dataset.zip(\n",
    "            (tf.data.Dataset.from_tensor_slices(patches),\n",
    "             tf.data.Dataset.from_tensor_slices(labels))\n",
    "        ))\n",
    "\n",
    "    # 2. Augmentation (Training Only)\n",
    "    if augment:\n",
    "        def augment_fn(patch, label):\n",
    "            patch = tf.image.random_flip_left_right(patch)\n",
    "            return patch, label\n",
    "        ds = ds.map(augment_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    # 3. Batching\n",
    "    ds = ds.batch(CONFIG[\"BATCH_SIZE\"]).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# Prepare Metadata\n",
    "if os.path.exists(CONFIG[\"CSV_PATH\"]):\n",
    "    labels_data = pd.read_csv(CONFIG[\"CSV_PATH\"])\n",
    "    \n",
    "    print(\"Creating Datasets...\")\n",
    "    train_ds = create_dataset(CONFIG[\"DATA_DIR\"], labels_data, 'train', augment=CONFIG[\"AUGMENT\"])\n",
    "    val_ds = create_dataset(CONFIG[\"DATA_DIR\"], labels_data, 'val', augment=False)\n",
    "    \n",
    "    # Test set includes IDs to aggregate patch predictions back to images later\n",
    "    test_ds = create_dataset(CONFIG[\"DATA_DIR\"], labels_data, 'test', augment=False, include_id=True)\n",
    "    print(\"Datasets Ready.\")\n",
    "else:\n",
    "    print(f\"Error: CSV not found at {CONFIG['CSV_PATH']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training (Two-Stage Transfer Learning)\n",
    "We define a helper function to instantiate SOTA models with the classification head removed. The training proceeds in two phases to preserve pre-trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7ZqNFhG2KKv"
   },
   "outputs": [],
   "source": [
    "def build_regression_model(base_model_class, input_shape=(400, 400, 3)):\n",
    "    \"\"\"\n",
    "    Wraps a pre-trained backbone with a regression head.\n",
    "    \"\"\"\n",
    "    base_model = base_model_class(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False) # Important: keep BatchNormalization in inference mode\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "flSxuFS_nNGd"
   },
   "outputs": [],
   "source": [
    "MODEL_ARCHITECTURES = {\n",
    "    'ResNet50': ResNet50,\n",
    "    'InceptionV3': InceptionV3,\n",
    "    'InceptionResNetV2': InceptionResNetV2,\n",
    "    'DenseNet121': DenseNet121,\n",
    "    'EfficientNetV2M': EfficientNetV2M\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "for name, architecture in MODEL_ARCHITECTURES.items():\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Training Model: {name}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    save_path = os.path.join(CONFIG[\"MODELS_DIR\"], f\"{name}_best.keras\")\n",
    "    \n",
    "    # Build Model\n",
    "    model, base_model = build_regression_model(architecture)\n",
    "    \n",
    "    # --- Phase 1: Frozen Backbone ---\n",
    "    print(f\"Phase 1: Frozen Training ({CONFIG['EPOCHS_FROZEN']} epochs)\")\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=CONFIG[\"LR_FROZEN\"]),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "    \n",
    "    callbacks = [\n",
    "        ModelCheckpoint(save_path, monitor='val_loss', save_best_only=True, verbose=0),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=0),\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)\n",
    "    ]\n",
    "    \n",
    "    model.fit(train_ds, validation_data=val_ds, epochs=CONFIG[\"EPOCHS_FROZEN\"], callbacks=callbacks)\n",
    "    \n",
    "    # --- Phase 2: Fine-Tuning ---\n",
    "    print(f\"Phase 2: Fine-Tuning ({CONFIG['EPOCHS_FINE_TUNE']} epochs)\")\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    # Recompile with lower learning rate\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=CONFIG[\"LR_FINE_TUNE\"]),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "    \n",
    "    model.fit(train_ds, validation_data=val_ds, epochs=CONFIG[\"EPOCHS_FINE_TUNE\"], callbacks=callbacks)\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    print(f\"Finished {name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluation & Ensembling\n",
    "Since our dataset consists of patches, we must aggregate predictions:\n",
    "1.  **Patch Prediction:** Each model predicts the age for all patches of a test image.\n",
    "2.  **Ensemble Averaging:** We average the predictions of all 5 models for each patch.\n",
    "3.  **Image Aggregation:** We take the mean of all patch predictions belonging to a specific file ID to get the final Age estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, test_ds):\n",
    "    \"\"\"\n",
    "    Generates predictions using simple averaging of all models.\n",
    "    Returns list of (predicted_age, image_id, true_age)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(\"Generating Ensemble Predictions...\")\n",
    "    for batch in tqdm(test_ds):\n",
    "        patches, labels, ids = batch\n",
    "        \n",
    "        # Get predictions from all models\n",
    "        batch_preds = []\n",
    "        for model in models.values():\n",
    "            p = model.predict(patches, verbose=0)\n",
    "            batch_preds.append(p)\n",
    "        \n",
    "        # Average across models (Ensemble)\n",
    "        # Shape: (Num_Models, Batch, 1) -> Mean -> (Batch, 1)\n",
    "        avg_preds = np.mean(batch_preds, axis=0).flatten()\n",
    "        \n",
    "        # Store results\n",
    "        current_ids = [i.decode('utf-8') for i in ids.numpy()]\n",
    "        current_labels = labels.numpy()\n",
    "        \n",
    "        for pred, file_id, true_val in zip(avg_preds, current_ids, current_labels):\n",
    "            results.append((pred, file_id, true_val))\n",
    "            \n",
    "    return results\n",
    "\n",
    "def aggregate_predictions(raw_results):\n",
    "    \"\"\"\n",
    "    Aggregates patch-level predictions back to image-level using the mean.\n",
    "    \"\"\"\n",
    "    img_preds = defaultdict(list)\n",
    "    img_truth = {}\n",
    "    \n",
    "    for pred, file_id, true_val in raw_results:\n",
    "        img_preds[file_id].append(pred)\n",
    "        img_truth[file_id] = true_val\n",
    "        \n",
    "    final_preds = []\n",
    "    final_truth = []\n",
    "    \n",
    "    for file_id in img_preds:\n",
    "        # Mean of all patches for this image\n",
    "        final_preds.append(np.mean(img_preds[file_id]))\n",
    "        final_truth.append(img_truth[file_id])\n",
    "        \n",
    "    return np.array(final_truth), np.array(final_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best models if not in memory\n",
    "# trained_models = { name: tf.keras.models.load_model(path) ... }\n",
    "\n",
    "# 1. Run Inference\n",
    "raw_results = ensemble_predict(trained_models, test_ds)\n",
    "\n",
    "# 2. Aggregate\n",
    "y_true, y_pred = aggregate_predictions(raw_results)\n",
    "\n",
    "# 3. Calculate Metrics\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "errors = np.abs(y_true - y_pred)\n",
    "within_5 = np.mean(errors <= 5) * 100\n",
    "\n",
    "print(f\"\\n--- Experiment 02 Results (SOTA Ensemble) ---\")\n",
    "print(f\"MAE:  {mae:.2f} years\")\n",
    "print(f\"RMSE: {rmse:.2f} years\")\n",
    "print(f\"R²:   {r2:.4f}\")\n",
    "print(f\"Accuracy (±5 years): {within_5:.2f}%\")\n",
    "\n",
    "# 4. Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(errors, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.title('Error Distribution (Ensemble)')\n",
    "plt.xlabel('Absolute Error (Years)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "0Uwh4zzlSjab"
   ],
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1E7XVe7s7krkBgHmHF1xHVQSW3-HA1wnS",
     "timestamp": 1737130270583
    },
    {
     "file_id": "14M5I5jvx32ttEF3HFGCvpBJ7mGBF2lWL",
     "timestamp": 1736670846297
    }
   ]
  },
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
